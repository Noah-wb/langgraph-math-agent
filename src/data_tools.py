"""
CSVæ•°æ®åˆ†æå·¥å…·æ¨¡å—
æä¾›åŸºç¡€å’Œé«˜ç´šæ•°æ®åˆ†æåŠŸèƒ½ï¼Œæ”¯æŒä¸­æ–‡åˆ—åå’Œå¤æ‚æ•°æ®ç»“æ„
"""

import os
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Union
from langchain.tools import tool
import warnings
import logging
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import seaborn as sns
from datetime import datetime
import markdown
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_RIGHT
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_RIGHT
from weasyprint import HTML, CSS

# å¿½ç•¥pandasè­¦å‘Š
warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)

# æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
DATA_DIR = "./data"

def _validate_file_path(filename: str) -> str:
    """éªŒè¯æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿æ–‡ä»¶åœ¨dataæ–‡ä»¶å¤¹å†…"""
    # ç§»é™¤è·¯å¾„åˆ†éš”ç¬¦ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»
    safe_filename = os.path.basename(filename)
    full_path = os.path.join(DATA_DIR, safe_filename)
    
    # ç¡®ä¿è·¯å¾„åœ¨dataæ–‡ä»¶å¤¹å†…
    if not os.path.abspath(full_path).startswith(os.path.abspath(DATA_DIR)):
        raise ValueError("æ–‡ä»¶è·¯å¾„ä¸å®‰å…¨ï¼Œåªèƒ½è®¿é—®dataæ–‡ä»¶å¤¹å†…çš„æ–‡ä»¶")
    
    return full_path

def _safe_read_csv(filename: str) -> pd.DataFrame:
    """å®‰å…¨è¯»å–CSVæ–‡ä»¶"""
    try:
        file_path = _validate_file_path(filename)
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        
        # è¯»å–CSVæ–‡ä»¶ï¼Œæ”¯æŒä¸­æ–‡ç¼–ç 
        df = pd.read_csv(file_path, encoding='utf-8')
        return df
    except Exception as e:
        raise Exception(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")

@tool
def list_csv_files() -> str:
    """
    åˆ—å‡ºdataæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶
    
    Returns:
        str: å¯ç”¨CSVæ–‡ä»¶åˆ—è¡¨çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        if not os.path.exists(DATA_DIR):
            return "dataæ–‡ä»¶å¤¹ä¸å­˜åœ¨"
        
        csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv')]
        
        if not csv_files:
            return "dataæ–‡ä»¶å¤¹ä¸­æ²¡æœ‰CSVæ–‡ä»¶"
        
        result = "ğŸ“ å¯ç”¨çš„CSVæ–‡ä»¶:\n"
        for i, file in enumerate(csv_files, 1):
            file_path = os.path.join(DATA_DIR, file)
            file_size = os.path.getsize(file_path)
            result += f"  {i}. {file} ({file_size:,} bytes)\n"
        
        return result
    except Exception as e:
        return f"âŒ åˆ—å‡ºCSVæ–‡ä»¶å¤±è´¥: {str(e)}"

@tool
def load_csv_file(filename: str) -> str:
    """
    åŠ è½½CSVæ–‡ä»¶å¹¶è¿”å›åŸºæœ¬ä¿¡æ¯
    
    Args:
        filename: CSVæ–‡ä»¶å
        
    Returns:
        str: æ–‡ä»¶åŸºæœ¬ä¿¡æ¯çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        df = _safe_read_csv(filename)
        
        result = f"ğŸ“Š æ–‡ä»¶: {filename}\n"
        result += f"ğŸ“ æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—\n"
        result += f"ğŸ“‹ åˆ—å: {', '.join(df.columns.tolist())}\n\n"
        
        # æ˜¾ç¤ºå‰5è¡Œæ•°æ®
        result += "ğŸ“„ å‰5è¡Œæ•°æ®é¢„è§ˆ:\n"
        preview = df.head().to_string(index=False)
        result += preview
        
        return result
    except Exception as e:
        return f"âŒ åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {str(e)}"

@tool
def get_column_info(filename: str, column_name: str) -> str:
    """
    è·å–æŒ‡å®šåˆ—çš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        filename: CSVæ–‡ä»¶å
        column_name: åˆ—å
        
    Returns:
        str: åˆ—è¯¦ç»†ä¿¡æ¯çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        df = _safe_read_csv(filename)
        
        if column_name not in df.columns:
            return f"âŒ åˆ— '{column_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        col = df[column_name]
        
        result = f"ğŸ“Š åˆ—å: {column_name}\n"
        result += f"ğŸ“ æ•°æ®ç±»å‹: {col.dtype}\n"
        result += f"ğŸ“ˆ éç©ºå€¼æ•°é‡: {col.count()}\n"
        result += f"âŒ ç¼ºå¤±å€¼æ•°é‡: {col.isnull().sum()}\n"
        result += f"ğŸ”¢ å”¯ä¸€å€¼æ•°é‡: {col.nunique()}\n"
        
        if col.dtype in ['int64', 'float64']:
            result += f"ğŸ“Š æ•°å€¼èŒƒå›´: {col.min()} ~ {col.max()}\n"
        
        # æ˜¾ç¤ºå‰10ä¸ªå”¯ä¸€å€¼
        unique_values = col.dropna().unique()[:10]
        result += f"ğŸ” å‰10ä¸ªå”¯ä¸€å€¼: {', '.join(map(str, unique_values))}\n"
        
        return result
    except Exception as e:
        return f"âŒ è·å–åˆ—ä¿¡æ¯å¤±è´¥: {str(e)}"

@tool
def get_column_stats(filename: str, column_name: str) -> str:
    """
    è·å–æ•°å€¼åˆ—çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        filename: CSVæ–‡ä»¶å
        column_name: åˆ—å
        
    Returns:
        str: ç»Ÿè®¡ä¿¡æ¯çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        df = _safe_read_csv(filename)
        
        if column_name not in df.columns:
            return f"âŒ åˆ— '{column_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        col = df[column_name]
        
        if not pd.api.types.is_numeric_dtype(col):
            return f"âŒ åˆ— '{column_name}' ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œæ— æ³•è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"
        
        # ç§»é™¤ç¼ºå¤±å€¼
        col_clean = col.dropna()
        
        if len(col_clean) == 0:
            return f"âŒ åˆ— '{column_name}' æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼æ•°æ®"
        
        result = f"ğŸ“Š åˆ— '{column_name}' ç»Ÿè®¡ä¿¡æ¯:\n"
        result += f"ğŸ“ æ•°æ®ç‚¹æ•°é‡: {len(col_clean)}\n"
        result += f"â• æ€»å’Œ: {col_clean.sum():,.2f}\n"
        result += f"ğŸ“Š å¹³å‡å€¼: {col_clean.mean():,.2f}\n"
        result += f"ğŸ“ˆ ä¸­ä½æ•°: {col_clean.median():,.2f}\n"
        result += f"ğŸ“‰ æœ€å°å€¼: {col_clean.min():,.2f}\n"
        result += f"ğŸ“ˆ æœ€å¤§å€¼: {col_clean.max():,.2f}\n"
        result += f"ğŸ“Š æ ‡å‡†å·®: {col_clean.std():,.2f}\n"
        result += f"ğŸ“Š æ–¹å·®: {col_clean.var():,.2f}\n"
        
        return result
    except Exception as e:
        return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"

@tool
def calculate_summary(filename: str) -> str:
    """
    è®¡ç®—æ‰€æœ‰æ•°å€¼åˆ—çš„æ±‡æ€»ç»Ÿè®¡
    
    Args:
        filename: CSVæ–‡ä»¶å
        
    Returns:
        str: æ±‡æ€»ç»Ÿè®¡çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        df = _safe_read_csv(filename)
        
        # è·å–æ•°å€¼åˆ—
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if not numeric_cols:
            return "âŒ æ–‡ä»¶ä¸­æ²¡æœ‰æ•°å€¼åˆ—"
        
        result = f"ğŸ“Š æ•°å€¼åˆ—æ±‡æ€»ç»Ÿè®¡ (å…±{len(numeric_cols)}åˆ—):\n\n"
        
        for col in numeric_cols:
            col_data = df[col].dropna()
            if len(col_data) > 0:
                result += f"ğŸ“ˆ {col}:\n"
                result += f"  æ€»å’Œ: {col_data.sum():,.2f}\n"
                result += f"  å¹³å‡å€¼: {col_data.mean():,.2f}\n"
                result += f"  ä¸­ä½æ•°: {col_data.median():,.2f}\n"
                result += f"  æ ‡å‡†å·®: {col_data.std():,.2f}\n\n"
        
        return result
    except Exception as e:
        return f"âŒ è®¡ç®—æ±‡æ€»ç»Ÿè®¡å¤±è´¥: {str(e)}"

@tool
def get_unique_values(filename: str, column_name: str) -> str:
    """
    è·å–åˆ—çš„æ‰€æœ‰å”¯ä¸€å€¼
    
    Args:
        filename: CSVæ–‡ä»¶å
        column_name: åˆ—å
        
    Returns:
        str: å”¯ä¸€å€¼åˆ—è¡¨çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        df = _safe_read_csv(filename)
        
        if column_name not in df.columns:
            return f"âŒ åˆ— '{column_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        unique_vals = df[column_name].dropna().unique()
        
        result = f"ğŸ” åˆ— '{column_name}' çš„å”¯ä¸€å€¼ (å…±{len(unique_vals)}ä¸ª):\n"
        
        # å¦‚æœå”¯ä¸€å€¼å¤ªå¤šï¼Œåªæ˜¾ç¤ºå‰20ä¸ª
        if len(unique_vals) > 20:
            result += f"æ˜¾ç¤ºå‰20ä¸ªå”¯ä¸€å€¼:\n"
            for val in unique_vals[:20]:
                result += f"  â€¢ {val}\n"
            result += f"... è¿˜æœ‰ {len(unique_vals) - 20} ä¸ªå€¼"
        else:
            for val in unique_vals:
                result += f"  â€¢ {val}\n"
        
        return result
    except Exception as e:
        return f"âŒ è·å–å”¯ä¸€å€¼å¤±è´¥: {str(e)}"

@tool
def filter_data(filename: str, column_name: str, operator: str, value: Union[str, int, float]) -> str:
    """
    æ ¹æ®æ¡ä»¶ç­›é€‰æ•°æ®
    
    Args:
        filename: CSVæ–‡ä»¶å
        column_name: åˆ—å
        operator: æ“ä½œç¬¦ (>, <, =, >=, <=, !=, contains)
        value: æ¯”è¾ƒå€¼
        
    Returns:
        str: ç­›é€‰ç»“æœçš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        df = _safe_read_csv(filename)
        
        if column_name not in df.columns:
            return f"âŒ åˆ— '{column_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        # æ ¹æ®æ“ä½œç¬¦ç­›é€‰æ•°æ®
        if operator == ">":
            filtered_df = df[df[column_name] > value]
        elif operator == "<":
            filtered_df = df[df[column_name] < value]
        elif operator == "=":
            filtered_df = df[df[column_name] == value]
        elif operator == ">=":
            filtered_df = df[df[column_name] >= value]
        elif operator == "<=":
            filtered_df = df[df[column_name] <= value]
        elif operator == "!=":
            filtered_df = df[df[column_name] != value]
        elif operator == "contains":
            filtered_df = df[df[column_name].astype(str).str.contains(str(value), na=False)]
        else:
            return f"âŒ ä¸æ”¯æŒçš„æ“ä½œç¬¦: {operator}ã€‚æ”¯æŒçš„æ“ä½œç¬¦: >, <, =, >=, <=, !=, contains"
        
        result = f"ğŸ” ç­›é€‰æ¡ä»¶: {column_name} {operator} {value}\n"
        result += f"ğŸ“Š ç­›é€‰ç»“æœ: {len(filtered_df)} è¡Œ (åŸæ•°æ® {len(df)} è¡Œ)\n\n"
        
        if len(filtered_df) > 0:
            result += "ğŸ“„ ç­›é€‰ç»“æœé¢„è§ˆ (å‰10è¡Œ):\n"
            preview = filtered_df.head(10).to_string(index=False)
            result += preview
        else:
            result += "âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®"
        
        return result
    except Exception as e:
        return f"âŒ ç­›é€‰æ•°æ®å¤±è´¥: {str(e)}"

@tool
def group_by_sum(filename: str, group_column: str, sum_column: str) -> str:
    """
    æŒ‰åˆ—åˆ†ç»„å¹¶æ±‚å’Œ
    
    Args:
        filename: CSVæ–‡ä»¶å
        group_column: åˆ†ç»„åˆ—å
        sum_column: æ±‚å’Œåˆ—å
        
    Returns:
        str: åˆ†ç»„æ±‚å’Œç»“æœçš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        df = _safe_read_csv(filename)
        
        if group_column not in df.columns:
            return f"âŒ åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        if sum_column not in df.columns:
            return f"âŒ æ±‚å’Œåˆ— '{sum_column}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        if not pd.api.types.is_numeric_dtype(df[sum_column]):
            return f"âŒ æ±‚å’Œåˆ— '{sum_column}' ä¸æ˜¯æ•°å€¼ç±»å‹"
        
        # åˆ†ç»„æ±‚å’Œ
        grouped = df.groupby(group_column)[sum_column].sum().reset_index()
        grouped = grouped.sort_values(sum_column, ascending=False)
        
        result = f"ğŸ“Š æŒ‰ '{group_column}' åˆ†ç»„ï¼Œå¯¹ '{sum_column}' æ±‚å’Œ:\n\n"
        result += grouped.to_string(index=False)
        
        result += f"\n\nğŸ“ˆ æ€»è®¡: {grouped[sum_column].sum():,.2f}"
        
        return result
    except Exception as e:
        return f"âŒ åˆ†ç»„æ±‚å’Œå¤±è´¥: {str(e)}"

@tool
def group_by_aggregate(filename: str, group_column: str, agg_column: str, agg_function: str) -> str:
    """
    åˆ†ç»„èšåˆåˆ†æ
    
    Args:
        filename: CSVæ–‡ä»¶å
        group_column: åˆ†ç»„åˆ—å
        agg_column: èšåˆåˆ—å
        agg_function: èšåˆå‡½æ•° (sum, mean, count, max, min)
        
    Returns:
        str: èšåˆç»“æœçš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        df = _safe_read_csv(filename)
        
        if group_column not in df.columns:
            return f"âŒ åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        if agg_column not in df.columns:
            return f"âŒ èšåˆåˆ— '{agg_column}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        if agg_function not in ['sum', 'mean', 'count', 'max', 'min']:
            return f"âŒ ä¸æ”¯æŒçš„èšåˆå‡½æ•°: {agg_function}ã€‚æ”¯æŒçš„å‡½æ•°: sum, mean, count, max, min"
        
        # æ‰§è¡Œèšåˆ
        if agg_function == 'sum':
            result_series = df.groupby(group_column)[agg_column].sum()
        elif agg_function == 'mean':
            result_series = df.groupby(group_column)[agg_column].mean()
        elif agg_function == 'count':
            result_series = df.groupby(group_column)[agg_column].count()
        elif agg_function == 'max':
            result_series = df.groupby(group_column)[agg_column].max()
        elif agg_function == 'min':
            result_series = df.groupby(group_column)[agg_column].min()
        
        result_df = result_series.reset_index()
        result_df = result_df.sort_values(agg_column, ascending=False)
        
        result = f"ğŸ“Š æŒ‰ '{group_column}' åˆ†ç»„ï¼Œå¯¹ '{agg_column}' æ‰§è¡Œ {agg_function} èšåˆ:\n\n"
        result += result_df.to_string(index=False)
        
        return result
    except Exception as e:
        return f"âŒ åˆ†ç»„èšåˆå¤±è´¥: {str(e)}"

@tool
def calculate_correlation(filename: str, column1: str, column2: str) -> str:
    """
    è®¡ç®—ä¸¤ä¸ªæ•°å€¼åˆ—çš„ç›¸å…³ç³»æ•°
    
    Args:
        filename: CSVæ–‡ä»¶å
        column1: ç¬¬ä¸€ä¸ªåˆ—å
        column2: ç¬¬äºŒä¸ªåˆ—å
        
    Returns:
        str: ç›¸å…³ç³»æ•°çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        df = _safe_read_csv(filename)
        
        if column1 not in df.columns:
            return f"âŒ åˆ— '{column1}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        if column2 not in df.columns:
            return f"âŒ åˆ— '{column2}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        if not pd.api.types.is_numeric_dtype(df[column1]):
            return f"âŒ åˆ— '{column1}' ä¸æ˜¯æ•°å€¼ç±»å‹"
        
        if not pd.api.types.is_numeric_dtype(df[column2]):
            return f"âŒ åˆ— '{column2}' ä¸æ˜¯æ•°å€¼ç±»å‹"
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = df[column1].corr(df[column2])
        
        result = f"ğŸ“Š åˆ— '{column1}' å’Œ '{column2}' çš„ç›¸å…³ç³»æ•°:\n"
        result += f"ğŸ”— ç›¸å…³ç³»æ•°: {correlation:.4f}\n\n"
        
        # è§£é‡Šç›¸å…³ç³»æ•°
        if abs(correlation) >= 0.8:
            strength = "å¼º"
        elif abs(correlation) >= 0.5:
            strength = "ä¸­ç­‰"
        elif abs(correlation) >= 0.3:
            strength = "å¼±"
        else:
            strength = "å¾ˆå¼±"
        
        direction = "æ­£" if correlation > 0 else "è´Ÿ"
        result += f"ğŸ“ˆ ç›¸å…³æ€§: {strength}{direction}ç›¸å…³"
        
        return result
    except Exception as e:
        return f"âŒ è®¡ç®—ç›¸å…³ç³»æ•°å¤±è´¥: {str(e)}"

@tool
def get_top_n_rows(filename: str, column_name: str, n: int, ascending: bool = False) -> str:
    """
    æŒ‰æŒ‡å®šåˆ—æ’åºè·å–å‰Nè¡Œ
    
    Args:
        filename: CSVæ–‡ä»¶å
        column_name: æ’åºåˆ—å
        n: è¿”å›è¡Œæ•°
        ascending: æ˜¯å¦å‡åºæ’åˆ— (é»˜è®¤Falseï¼Œå³é™åº)
        
    Returns:
        str: æ’åºç»“æœçš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        df = _safe_read_csv(filename)
        
        if column_name not in df.columns:
            return f"âŒ åˆ— '{column_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        if not pd.api.types.is_numeric_dtype(df[column_name]):
            return f"âŒ åˆ— '{column_name}' ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œæ— æ³•æ’åº"
        
        # æ’åºå¹¶è·å–å‰Nè¡Œ
        sorted_df = df.sort_values(column_name, ascending=ascending).head(n)
        
        order = "å‡åº" if ascending else "é™åº"
        result = f"ğŸ“Š æŒ‰ '{column_name}' {order}æ’åˆ—çš„å‰{n}è¡Œ:\n\n"
        result += sorted_df.to_string(index=False)
        
        return result
    except Exception as e:
        return f"âŒ è·å–æ’åºç»“æœå¤±è´¥: {str(e)}"

@tool
def search_rows(filename: str, column_name: str, keyword: str) -> str:
    """
    åœ¨åˆ—ä¸­æœç´¢åŒ…å«å…³é”®è¯çš„è¡Œ
    
    Args:
        filename: CSVæ–‡ä»¶å
        column_name: æœç´¢åˆ—å
        keyword: æœç´¢å…³é”®è¯
        
    Returns:
        str: æœç´¢ç»“æœçš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        df = _safe_read_csv(filename)
        
        if column_name not in df.columns:
            return f"âŒ åˆ— '{column_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}"
        
        # æœç´¢åŒ…å«å…³é”®è¯çš„è¡Œ
        mask = df[column_name].astype(str).str.contains(keyword, case=False, na=False)
        search_results = df[mask]
        
        result = f"ğŸ” åœ¨åˆ— '{column_name}' ä¸­æœç´¢ '{keyword}':\n"
        result += f"ğŸ“Š æ‰¾åˆ° {len(search_results)} è¡ŒåŒ¹é…ç»“æœ\n\n"
        
        if len(search_results) > 0:
            result += "ğŸ“„ æœç´¢ç»“æœ:\n"
            result += search_results.to_string(index=False)
        else:
            result += "âŒ æ²¡æœ‰æ‰¾åˆ°åŒ…å«è¯¥å…³é”®è¯çš„è¡Œ"
        
        return result
    except Exception as e:
        return f"âŒ æœç´¢å¤±è´¥: {str(e)}"

@tool
def professional_data_analysis(filename: str, analysis_type: str = "comprehensive") -> str:
    """
    ä¸“ä¸šæ•°æ®åˆ†æå·¥å…·ï¼Œä¸“é—¨ç”¨äºIPTVä¸šåŠ¡æ•°æ®åˆ†æ
    
    Args:
        filename: CSVæ–‡ä»¶å
        analysis_type: åˆ†æç±»å‹ ("comprehensive", "daily_trend", "comparison", "summary")
    
    Returns:
        str: ä¸“ä¸šæ•°æ®åˆ†ææŠ¥å‘Š
    """
    try:
        df = _safe_read_csv(filename)
        
        if df.empty:
            return "âŒ æ•°æ®æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ"
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼å¹¶é€‚é…
        result = "ğŸ“Š ä¸“ä¸šæ•°æ®åˆ†ææŠ¥å‘Š\n"
        result += "=" * 50 + "\n\n"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºIPTVäº§å“åŒ…æ•°æ®æ ¼å¼
        if 'äº§å“åŒ…åç§°' in df.columns and 'äº§å“åŒ…åˆ†ç±»' in df.columns:
            # IPTVäº§å“åŒ…æ•°æ®æ ¼å¼
            return _analyze_iptv_product_data(df, analysis_type)
        else:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ—¶é—´åºåˆ—æ•°æ®æ ¼å¼
            required_columns = ['æ—¥æœŸ', 'äº§å“åŒ…åç§°', 'è®¢è´­é‡']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                return f"âŒ æ•°æ®æ ¼å¼ä¸æ”¯æŒï¼Œç¼ºå°‘å¿…è¦çš„åˆ—: {', '.join(missing_columns)}"
            return _analyze_time_series_data(df, analysis_type)
        
        
    except Exception as e:
        return f"âŒ ä¸“ä¸šæ•°æ®åˆ†æå¤±è´¥: {str(e)}"


def _analyze_iptv_product_data(df: pd.DataFrame, analysis_type: str) -> str:
    """åˆ†æIPTVäº§å“åŒ…æ•°æ® - ä½¿ç”¨AIæ¨¡å‹å’Œæç¤ºè¯è¿›è¡Œä¸“ä¸šåˆ†æ"""
    try:
        # è¯»å–æç¤ºè¯æ–‡ä»¶
        prompt_file = "prompts/data_analysis_prompt.txt"
        if os.path.exists(prompt_file):
            with open(prompt_file, 'r', encoding='utf-8') as f:
                prompt_template = f.read()
        else:
            # å¦‚æœæç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯
            prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¤§å¸ˆï¼Œä¸“æ³¨äºIPTVä¸šåŠ¡é¢†åŸŸã€‚è¯·å¯¹æä¾›çš„æ•°æ®è¿›è¡Œä¸“ä¸šåˆ†æï¼ŒåŒ…æ‹¬ï¼š
1. æ•°æ®èµ°åŠ¿åˆ†æ
2. åŒæ¯”ç¯æ¯”åˆ†æ  
3. æ€»ç»“æ€§åˆ†æ
è¯·æä¾›è¯¦ç»†ã€ä¸“ä¸šçš„åˆ†ææŠ¥å‘Šã€‚"""
        
        # å‡†å¤‡æ•°æ®æ‘˜è¦
        data_summary = _prepare_data_summary(df)
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        full_prompt = f"{prompt_template}\n\næ•°æ®æ‘˜è¦ï¼š\n{data_summary}\n\nè¯·æŒ‰ç…§æç¤ºè¯è¦æ±‚è¿›è¡Œä¸“ä¸šçš„æ•°æ®åˆ†æã€‚"
        
        # ä½¿ç”¨AIæ¨¡å‹è¿›è¡Œåˆ†æ
        from src.models import ModelManager
        model_manager = ModelManager()
        
        try:
            # åˆ‡æ¢åˆ°é»˜è®¤æ¨¡å‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è®¾ç½®ï¼‰
            if not model_manager.get_current_llm():
                # å°è¯•åˆ‡æ¢åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
                available_models = model_manager.get_available_models()
                if available_models:
                    model_manager.switch_model(available_models[0])
                else:
                    raise Exception("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
            
            # è·å–å½“å‰LLMå®ä¾‹
            llm = model_manager.get_current_llm()
            if not llm:
                raise Exception("æ— æ³•è·å–LLMå®ä¾‹")
            
            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [{"role": "user", "content": full_prompt}]
            
            # è°ƒç”¨æ¨¡å‹è¿›è¡Œåˆ†æ
            response = llm.invoke(messages)
            
            if response and hasattr(response, 'content') and response.content.strip():
                return f"ğŸ“Š ä¸“ä¸šæ•°æ®åˆ†ææŠ¥å‘Š\n{'='*50}\n\n{response.content}\n\n{'='*50}"
            else:
                # å¦‚æœAIåˆ†æå¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€åˆ†æ
                return _fallback_analysis(df)
                
        except Exception as e:
            logging.warning(f"AIæ¨¡å‹åˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ: {str(e)}")
            return _fallback_analysis(df)
            
    except Exception as e:
        logging.error(f"ä¸“ä¸šæ•°æ®åˆ†æå¤±è´¥: {str(e)}")
        return _fallback_analysis(df)


def _prepare_data_summary(df: pd.DataFrame) -> str:
    """å‡†å¤‡æ•°æ®æ‘˜è¦ä¾›AIåˆ†æä½¿ç”¨"""
    summary = []
    
    # åŸºç¡€ç»Ÿè®¡
    total_products = len(df)
    summary.append(f"äº§å“åŒ…æ€»æ•°: {total_products}")
    
    # åˆ†ç±»ç»Ÿè®¡
    if 'äº§å“åŒ…åˆ†ç±»' in df.columns:
        category_stats = df.groupby('äº§å“åŒ…åˆ†ç±»').agg({
            'äº§å“åŒ…åç§°': 'count',
            'äº§å“åŒ…å•ä»·ï¼ˆå…ƒï¼‰': ['mean', 'sum'],
            '2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰': 'sum',
            '2025å¹´åº¦6æœˆ æµæ°´ï¼ˆå…ƒï¼‰\nï¼ˆå•ä»·Ã—æ–°å¢è®¢è´­ï¼‰': 'sum'
        }).round(2)
        
        summary.append("\nåˆ†ç±»ç»Ÿè®¡:")
        for category in category_stats.index:
            count = category_stats.loc[category, ('äº§å“åŒ…åç§°', 'count')]
            avg_price = category_stats.loc[category, ('äº§å“åŒ…å•ä»·ï¼ˆå…ƒï¼‰', 'mean')]
            total_orders = category_stats.loc[category, ('2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰', 'sum')]
            total_revenue = category_stats.loc[category, ('2025å¹´åº¦6æœˆ æµæ°´ï¼ˆå…ƒï¼‰\nï¼ˆå•ä»·Ã—æ–°å¢è®¢è´­ï¼‰', 'sum')]
            
            summary.append(f"  {category}: äº§å“æ•°{count}, å¹³å‡å•ä»·Â¥{avg_price:.2f}, è®¢è´­é‡{total_orders:,}, æµæ°´Â¥{total_revenue:,.2f}")
    
    # çƒ­é—¨äº§å“
    if '2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰' in df.columns:
        top_products = df.nlargest(5, '2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰')
        summary.append("\nçƒ­é—¨äº§å“TOP5:")
        for i, (_, row) in enumerate(top_products.iterrows(), 1):
            summary.append(f"  {i}. {row['äº§å“åŒ…åç§°']}: è®¢è´­é‡{row['2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰']:,}")
    
    # å¢é•¿ç‡åˆ†æ
    if 'è®¢è´­åŒæ¯”å¢é•¿ç‡' in df.columns:
        growth_data = df[df['è®¢è´­åŒæ¯”å¢é•¿ç‡'] != '-100.00%'].copy()
        if not growth_data.empty:
            summary.append("\nåŒæ¯”å¢é•¿åˆ†æ:")
            positive_count = 0
            negative_count = 0
            for _, row in growth_data.iterrows():
                try:
                    rate = float(row['è®¢è´­åŒæ¯”å¢é•¿ç‡'].replace('%', ''))
                    if rate > 0:
                        positive_count += 1
                    elif rate < 0:
                        negative_count += 1
                except:
                    pass
            summary.append(f"  æ­£å¢é•¿: {positive_count}ä¸ª, è´Ÿå¢é•¿: {negative_count}ä¸ª")
    
    return "\n".join(summary)


def _fallback_analysis(df: pd.DataFrame) -> str:
    """å›é€€çš„åŸºç¡€åˆ†æï¼ˆåŸåˆ†æé€»è¾‘ï¼‰"""
    result = "ğŸ“Š IPTVäº§å“åŒ…æ•°æ®åˆ†ææŠ¥å‘Š\n"
    result += "=" * 50 + "\n\n"
    
    # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
    total_products = len(df)
    result += f"ğŸ“ˆ äº§å“åŒ…æ€»æ•°: {total_products}\n"
    
    # æŒ‰åˆ†ç±»åˆ†æ
    if 'äº§å“åŒ…åˆ†ç±»' in df.columns:
        category_analysis = df.groupby('äº§å“åŒ…åˆ†ç±»').agg({
            'äº§å“åŒ…åç§°': 'count',
            'äº§å“åŒ…å•ä»·ï¼ˆå…ƒï¼‰': ['mean', 'sum'],
            '2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰': 'sum',
            '2025å¹´åº¦6æœˆ æµæ°´ï¼ˆå…ƒï¼‰\nï¼ˆå•ä»·Ã—æ–°å¢è®¢è´­ï¼‰': 'sum'
        }).round(2)
        
        result += "\n1. äº§å“åŒ…åˆ†ç±»åˆ†æ\n"
        result += "-" * 30 + "\n"
        for category in category_analysis.index:
            count = category_analysis.loc[category, ('äº§å“åŒ…åç§°', 'count')]
            avg_price = category_analysis.loc[category, ('äº§å“åŒ…å•ä»·ï¼ˆå…ƒï¼‰', 'mean')]
            total_orders = category_analysis.loc[category, ('2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰', 'sum')]
            total_revenue = category_analysis.loc[category, ('2025å¹´åº¦6æœˆ æµæ°´ï¼ˆå…ƒï¼‰\nï¼ˆå•ä»·Ã—æ–°å¢è®¢è´­ï¼‰', 'sum')]
            
            result += f"ğŸ“¦ {category}:\n"
            result += f"   â€¢ äº§å“æ•°é‡: {count}\n"
            result += f"   â€¢ å¹³å‡å•ä»·: Â¥{avg_price:.2f}\n"
            result += f"   â€¢ æ€»è®¢è´­é‡: {total_orders:,}\n"
            result += f"   â€¢ æ€»æµæ°´: Â¥{total_revenue:,.2f}\n\n"
    
    # çƒ­é—¨äº§å“åˆ†æ
    if '2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰' in df.columns:
        top_products = df.nlargest(5, '2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰')
        result += "2. çƒ­é—¨äº§å“TOP5\n"
        result += "-" * 30 + "\n"
        for i, (_, row) in enumerate(top_products.iterrows(), 1):
            product_name = row['äº§å“åŒ…åç§°']
            orders = row['2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰']
            revenue = row['2025å¹´åº¦6æœˆ æµæ°´ï¼ˆå…ƒï¼‰\nï¼ˆå•ä»·Ã—æ–°å¢è®¢è´­ï¼‰']
            result += f"{i}. {product_name}\n"
            result += f"   â€¢ è®¢è´­é‡: {orders:,}\n"
            result += f"   â€¢ æµæ°´: Â¥{revenue:,.2f}\n\n"
    
    # åŒæ¯”å¢é•¿åˆ†æ
    if 'è®¢è´­åŒæ¯”å¢é•¿ç‡' in df.columns:
        growth_analysis = df[df['è®¢è´­åŒæ¯”å¢é•¿ç‡'] != '-100.00%'].copy()
        if not growth_analysis.empty:
            # è§£æå¢é•¿ç‡æ•°æ®
            growth_rates = []
            for rate_str in growth_analysis['è®¢è´­åŒæ¯”å¢é•¿ç‡']:
                try:
                    rate = float(rate_str.replace('%', ''))
                    growth_rates.append(rate)
                except:
                    growth_rates.append(0)
            
            growth_analysis['growth_rate_num'] = growth_rates
            positive_growth = growth_analysis[growth_analysis['growth_rate_num'] > 0]
            negative_growth = growth_analysis[growth_analysis['growth_rate_num'] < 0]
            
            result += "3. åŒæ¯”å¢é•¿åˆ†æ\n"
            result += "-" * 30 + "\n"
            result += f"ğŸ“ˆ æ­£å¢é•¿äº§å“: {len(positive_growth)}ä¸ª\n"
            result += f"ğŸ“‰ è´Ÿå¢é•¿äº§å“: {len(negative_growth)}ä¸ª\n\n"
            
            if not positive_growth.empty:
                top_growth = positive_growth.nlargest(3, 'growth_rate_num')
                result += "å¢é•¿æœ€å¿«äº§å“TOP3:\n"
                for i, (_, row) in enumerate(top_growth.iterrows(), 1):
                    result += f"{i}. {row['äº§å“åŒ…åç§°']}: +{row['growth_rate_num']:.1f}%\n"
                result += "\n"
    
    result += "=" * 50
    return result


def _analyze_time_series_data(df: pd.DataFrame, analysis_type: str) -> str:
    """åˆ†ææ—¶é—´åºåˆ—æ•°æ®"""
    result = "ğŸ“Š æ—¶é—´åºåˆ—æ•°æ®åˆ†ææŠ¥å‘Š\n"
    result += "=" * 50 + "\n\n"
    
    # æ•°æ®é¢„å¤„ç†
    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
    df = df.sort_values('æ—¥æœŸ')
    
    # è·å–æœ€æ–°ä¸€å¤©çš„æ•°æ®
    latest_date = df['æ—¥æœŸ'].max()
    latest_data = df[df['æ—¥æœŸ'] == latest_date]
    
    # è®¡ç®—å½“æœˆæ•°æ®
    current_month = latest_date.month
    current_year = latest_date.year
    monthly_data = df[(df['æ—¥æœŸ'].dt.month == current_month) & 
                     (df['æ—¥æœŸ'].dt.year == current_year)]
    
    # è®¡ç®—æ¯æ—¥å¹³å‡å€¼
    days_in_month = monthly_data['æ—¥æœŸ'].nunique()
    monthly_total = monthly_data['è®¢è´­é‡'].sum()
    daily_average = monthly_total / days_in_month if days_in_month > 0 else 0
    
    result += f"ğŸ“… åˆ†ææ—¥æœŸ: {latest_date.strftime('%Y-%m-%d')}\n"
    result += f"ğŸ“ˆ å½“æœˆè®¢è´­æ€»é‡: {monthly_total:,}\n"
    result += f"ğŸ“Š å½“æœˆæ¯æ—¥å¹³å‡è®¢è´­é‡: {daily_average:.2f}\n"
    result += f"ğŸ“… å½“æœˆåˆ†æå¤©æ•°: {days_in_month}å¤©\n\n"
    
    # å„ç±»å‹äº§å“åŒ…åˆ†æ
    if 'äº§å“åŒ…ç±»å‹' in df.columns:
        type_analysis = latest_data.groupby('äº§å“åŒ…ç±»å‹')['è®¢è´­é‡'].sum().sort_values(ascending=False)
        result += "1. å„ç±»å‹äº§å“åŒ…è®¢è´­é‡åˆ†æ:\n"
        for ptype, amount in type_analysis.items():
            result += f"   â€¢ {ptype}: {amount:,} è®¢è´­é‡\n"
        result += "\n"
    
    result += "=" * 50
    return result


def generate_charts_for_analysis(filename: str, output_dir: str = "sjfx") -> List[str]:
    """
    ä¸ºæ•°æ®åˆ†æç”Ÿæˆå›¾è¡¨
    
    Args:
        filename: CSVæ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        List[str]: ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        df = _safe_read_csv(filename)
        if df.empty:
            return []
        
        chart_files = []
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºIPTVäº§å“åŒ…æ•°æ®æ ¼å¼
        if 'äº§å“åŒ…åç§°' in df.columns and 'äº§å“åŒ…åˆ†ç±»' in df.columns:
            chart_files.extend(_generate_iptv_charts(df, output_dir))
        else:
            chart_files.extend(_generate_time_series_charts(df, output_dir))
        
        return chart_files
        
    except Exception as e:
        logging.error(f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {str(e)}")
        return []


def _generate_iptv_charts(df: pd.DataFrame, output_dir: str) -> List[str]:
    """ç”ŸæˆIPTVäº§å“åŒ…æ•°æ®å›¾è¡¨"""
    chart_files = []
    
    try:
        # 1. äº§å“åŒ…åˆ†ç±»è®¢è´­é‡é¥¼å›¾
        if 'äº§å“åŒ…åˆ†ç±»' in df.columns and '2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰' in df.columns:
            category_orders = df.groupby('äº§å“åŒ…åˆ†ç±»')['2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰'].sum()
            
            plt.figure(figsize=(10, 8))
            colors = plt.cm.Set3(np.linspace(0, 1, len(category_orders)))
            wedges, texts, autotexts = plt.pie(category_orders.values, 
                                              labels=category_orders.index,
                                              autopct='%1.1f%%',
                                              colors=colors,
                                              startangle=90)
            
            # ç¾åŒ–æ–‡æœ¬
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
            
            plt.title('äº§å“åŒ…åˆ†ç±»è®¢è´­é‡åˆ†å¸ƒ', fontsize=16, fontweight='bold', pad=20)
            plt.axis('equal')
            
            chart_file = os.path.join(output_dir, 'category_orders_pie.png')
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            chart_files.append(chart_file)
        
        # 2. çƒ­é—¨äº§å“TOP10æŸ±çŠ¶å›¾
        if '2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰' in df.columns:
            top_products = df.nlargest(10, '2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰')
            
            plt.figure(figsize=(12, 8))
            bars = plt.bar(range(len(top_products)), top_products['2025å¹´åº¦6æœˆ æ–°å¢è®¢è´­ï¼ˆå•ï¼‰'])
            plt.xlabel('äº§å“åŒ…', fontsize=12)
            plt.ylabel('è®¢è´­é‡ï¼ˆå•ï¼‰', fontsize=12)
            plt.title('çƒ­é—¨äº§å“TOP10è®¢è´­é‡', fontsize=16, fontweight='bold')
            
            # è®¾ç½®xè½´æ ‡ç­¾
            plt.xticks(range(len(top_products)), 
                      [name[:10] + '...' if len(name) > 10 else name 
                       for name in top_products['äº§å“åŒ…åç§°']], 
                      rotation=45, ha='right')
            
            # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, bar in enumerate(bars):
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{int(height):,}', ha='center', va='bottom', fontsize=10)
            
            plt.tight_layout()
            chart_file = os.path.join(output_dir, 'top_products_bar.png')
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            chart_files.append(chart_file)
        
        # 3. ä»·æ ¼åˆ†å¸ƒç›´æ–¹å›¾
        if 'äº§å“åŒ…å•ä»·ï¼ˆå…ƒï¼‰' in df.columns:
            plt.figure(figsize=(10, 6))
            plt.hist(df['äº§å“åŒ…å•ä»·ï¼ˆå…ƒï¼‰'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
            plt.xlabel('äº§å“åŒ…å•ä»·ï¼ˆå…ƒï¼‰', fontsize=12)
            plt.ylabel('äº§å“æ•°é‡', fontsize=12)
            plt.title('äº§å“åŒ…ä»·æ ¼åˆ†å¸ƒ', fontsize=16, fontweight='bold')
            plt.grid(True, alpha=0.3)
            
            chart_file = os.path.join(output_dir, 'price_distribution.png')
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            chart_files.append(chart_file)
        
        # 4. åŒæ¯”å¢é•¿ç‡åˆ†æ
        if 'è®¢è´­åŒæ¯”å¢é•¿ç‡' in df.columns:
            # è§£æå¢é•¿ç‡æ•°æ®
            growth_rates = []
            valid_products = []
            for _, row in df.iterrows():
                try:
                    rate_str = str(row['è®¢è´­åŒæ¯”å¢é•¿ç‡'])
                    if rate_str != '-100.00%' and rate_str != 'nan':
                        rate = float(rate_str.replace('%', ''))
                        growth_rates.append(rate)
                        valid_products.append(row['äº§å“åŒ…åç§°'])
                except:
                    continue
            
            if growth_rates:
                plt.figure(figsize=(12, 8))
                colors = ['green' if x > 0 else 'red' for x in growth_rates]
                bars = plt.bar(range(len(growth_rates)), growth_rates, color=colors, alpha=0.7)
                
                plt.xlabel('äº§å“åŒ…', fontsize=12)
                plt.ylabel('åŒæ¯”å¢é•¿ç‡ (%)', fontsize=12)
                plt.title('äº§å“åŒ…åŒæ¯”å¢é•¿ç‡åˆ†æ', fontsize=16, fontweight='bold')
                plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
                
                # è®¾ç½®xè½´æ ‡ç­¾
                plt.xticks(range(len(valid_products)), 
                          [name[:8] + '...' if len(name) > 8 else name 
                           for name in valid_products], 
                          rotation=45, ha='right')
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for i, (bar, rate) in enumerate(zip(bars, growth_rates)):
                    height = bar.get_height()
                    plt.text(bar.get_x() + bar.get_width()/2., 
                            height + (1 if height >= 0 else -3),
                            f'{rate:.1f}%', ha='center', 
                            va='bottom' if height >= 0 else 'top', fontsize=9)
                
                plt.tight_layout()
                chart_file = os.path.join(output_dir, 'growth_rate_analysis.png')
                plt.savefig(chart_file, dpi=300, bbox_inches='tight')
                plt.close()
                chart_files.append(chart_file)
        
    except Exception as e:
        logging.error(f"ç”ŸæˆIPTVå›¾è¡¨å¤±è´¥: {str(e)}")
    
    return chart_files


def _generate_time_series_charts(df: pd.DataFrame, output_dir: str) -> List[str]:
    """ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®å›¾è¡¨"""
    chart_files = []
    
    try:
        # æ—¶é—´åºåˆ—è¶‹åŠ¿å›¾
        if 'æ—¥æœŸ' in df.columns and 'è®¢è´­é‡' in df.columns:
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
            daily_orders = df.groupby('æ—¥æœŸ')['è®¢è´­é‡'].sum().sort_index()
            
            plt.figure(figsize=(12, 6))
            plt.plot(daily_orders.index, daily_orders.values, marker='o', linewidth=2, markersize=6)
            plt.xlabel('æ—¥æœŸ', fontsize=12)
            plt.ylabel('è®¢è´­é‡', fontsize=12)
            plt.title('æ¯æ—¥è®¢è´­é‡è¶‹åŠ¿', fontsize=16, fontweight='bold')
            plt.grid(True, alpha=0.3)
            plt.xticks(rotation=45)
            
            chart_file = os.path.join(output_dir, 'daily_trend.png')
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            chart_files.append(chart_file)
    
    except Exception as e:
        logging.error(f"ç”Ÿæˆæ—¶é—´åºåˆ—å›¾è¡¨å¤±è´¥: {str(e)}")
    
    return chart_files


def _generate_text_analysis(filename: str) -> str:
    """ç”Ÿæˆæ–‡æœ¬åˆ†æå†…å®¹ï¼ˆéLangChainå·¥å…·ç‰ˆæœ¬ï¼‰"""
    try:
        df = _safe_read_csv(filename)
        if df.empty:
            return "æ•°æ®æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–ã€‚"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºIPTVäº§å“åŒ…æ•°æ®æ ¼å¼
        if 'äº§å“åŒ…åç§°' in df.columns and 'äº§å“åŒ…åˆ†ç±»' in df.columns:
            return _analyze_iptv_product_data(df, "comprehensive")
        else:
            return _analyze_time_series_data(df, "comprehensive")
            
    except Exception as e:
        logging.error(f"ç”Ÿæˆæ–‡æœ¬åˆ†æå¤±è´¥: {str(e)}")
        return f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"


def generate_analysis_report(filename: str, output_dir: str = "sjfx") -> str:
    """
    ç”Ÿæˆå®Œæ•´çš„æ•°æ®åˆ†ææŠ¥å‘Šï¼ˆåŒ…å«å›¾è¡¨å’ŒMDæ–‡ä»¶ï¼‰
    
    Args:
        filename: CSVæ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡æœ¬åˆ†æ
        text_analysis = _generate_text_analysis(filename)
        
        # ç”Ÿæˆå›¾è¡¨
        chart_files = generate_charts_for_analysis(filename, output_dir)
        
        # ç”ŸæˆMDæŠ¥å‘Š
        report_content = _generate_md_report(filename, text_analysis, chart_files)
        
        # ä¿å­˜MDæ–‡ä»¶
        report_file = os.path.join(output_dir, f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logging.info(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file
        
    except Exception as e:
        logging.error(f"ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {str(e)}")
        return ""


def _generate_md_report(filename: str, text_analysis: str, chart_files: List[str]) -> str:
    """ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š"""
    report = f"""# æ•°æ®åˆ†ææŠ¥å‘Š

## æ•°æ®æ–‡ä»¶ä¿¡æ¯
- **æ–‡ä»¶å**: {os.path.basename(filename)}
- **åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ•°æ®è¡Œæ•°**: {len(_safe_read_csv(filename))}

## åˆ†æç»“æœ

{text_analysis}

## å¯è§†åŒ–å›¾è¡¨

"""
    
    # æ·»åŠ å›¾è¡¨
    if chart_files:
        for chart_file in chart_files:
            chart_name = os.path.basename(chart_file)
            report += f"### {chart_name.replace('.png', '').replace('_', ' ').title()}\n\n"
            report += f"![{chart_name}]({chart_name})\n\n"
    else:
        report += "æš‚æ— å›¾è¡¨ç”Ÿæˆã€‚\n\n"
    
    report += """## æ€»ç»“

æœ¬æŠ¥å‘ŠåŸºäºæä¾›çš„æ•°æ®æ–‡ä»¶è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼ŒåŒ…æ‹¬æ•°æ®æ¦‚è§ˆã€è¶‹åŠ¿åˆ†æã€å¼‚å¸¸æ£€æµ‹å’Œå¯è§†åŒ–å±•ç¤ºã€‚å¦‚éœ€æ›´è¯¦ç»†çš„åˆ†ææˆ–æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶è”ç³»ã€‚

---
*æŠ¥å‘Šç”±æ•°æ®åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
    
    return report


@tool
def generate_pdf_report(filename: str, output_dir: str = "test_output", report_title: str = "æ•°æ®åˆ†ææŠ¥å‘Š") -> str:
    """
    ç”ŸæˆPDFæ ¼å¼çš„æ•°æ®åˆ†ææŠ¥å‘Š
    
    Args:
        filename: CSVæ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
        report_title: æŠ¥å‘Šæ ‡é¢˜
    
    Returns:
        str: PDFæ–‡ä»¶è·¯å¾„
    """
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡æœ¬åˆ†æ
        text_analysis = _generate_text_analysis(filename)
        
        # ç”Ÿæˆå›¾è¡¨
        chart_files = generate_charts_for_analysis(filename, output_dir)
        
        # ç”ŸæˆPDFæ–‡ä»¶
        pdf_file = _generate_pdf_with_reportlab(filename, text_analysis, chart_files, output_dir, report_title)
        
        return pdf_file
        
    except Exception as e:
        logging.error(f"ç”ŸæˆPDFæŠ¥å‘Šå¤±è´¥: {str(e)}")
        return f"âŒ ç”ŸæˆPDFæŠ¥å‘Šå¤±è´¥: {str(e)}"


@tool
def generate_html_to_pdf_report(filename: str, output_dir: str = "test_output", report_title: str = "æ•°æ®åˆ†ææŠ¥å‘Š") -> str:
    """
    ä½¿ç”¨HTML+CSSç”ŸæˆPDFæ ¼å¼çš„æ•°æ®åˆ†ææŠ¥å‘Šï¼ˆæ”¯æŒæ›´ä¸°å¯Œçš„æ ·å¼ï¼‰
    
    Args:
        filename: CSVæ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
        report_title: æŠ¥å‘Šæ ‡é¢˜
    
    Returns:
        str: PDFæ–‡ä»¶è·¯å¾„
    """
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡æœ¬åˆ†æ
        text_analysis = _generate_text_analysis(filename)
        
        # ç”Ÿæˆå›¾è¡¨
        chart_files = generate_charts_for_analysis(filename, output_dir)
        
        # ç”ŸæˆHTMLå†…å®¹
        html_content = _generate_html_report(filename, text_analysis, chart_files, report_title)
        
        # è½¬æ¢ä¸ºPDF
        pdf_file = _convert_html_to_pdf(html_content, output_dir, report_title)
        
        return pdf_file
        
    except Exception as e:
        logging.error(f"ç”ŸæˆHTMLè½¬PDFæŠ¥å‘Šå¤±è´¥: {str(e)}")
        return f"âŒ ç”ŸæˆHTMLè½¬PDFæŠ¥å‘Šå¤±è´¥: {str(e)}"


def _generate_pdf_with_reportlab(filename: str, text_analysis: str, chart_files: List[str], output_dir: str, report_title: str) -> str:
    """ä½¿ç”¨ReportLabç”ŸæˆPDFæŠ¥å‘Š"""
    try:
        # åˆ›å»ºPDFæ–‡ä»¶
        pdf_filename = f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        pdf_path = os.path.join(output_dir, pdf_filename)
        
        # åˆ›å»ºPDFæ–‡æ¡£
        doc = SimpleDocTemplate(pdf_path, pagesize=A4)
        story = []
        
        # è·å–æ ·å¼
        styles = getSampleStyleSheet()
        
        # åˆ›å»ºè‡ªå®šä¹‰æ ·å¼
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=18,
            spaceAfter=30,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        )
        
        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontSize=14,
            spaceAfter=12,
            textColor=colors.darkblue
        )
        
        # æ·»åŠ æ ‡é¢˜
        story.append(Paragraph(report_title, title_style))
        story.append(Spacer(1, 20))
        
        # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
        story.append(Paragraph("æ•°æ®æ–‡ä»¶ä¿¡æ¯", heading_style))
        file_info = f"""
        <b>æ–‡ä»¶å:</b> {os.path.basename(filename)}<br/>
        <b>åˆ†ææ—¶é—´:</b> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br/>
        <b>æ•°æ®è¡Œæ•°:</b> {len(_safe_read_csv(filename))}
        """
        story.append(Paragraph(file_info, styles['Normal']))
        story.append(Spacer(1, 20))
        
        # æ·»åŠ åˆ†æç»“æœ
        story.append(Paragraph("åˆ†æç»“æœ", heading_style))
        
        # å°†åˆ†æç»“æœåˆ†æ®µæ·»åŠ åˆ°PDF
        analysis_lines = text_analysis.split('\n')
        for line in analysis_lines:
            if line.strip():
                if line.startswith('ğŸ“Š') or line.startswith('ğŸ“ˆ') or line.startswith('ğŸ“‰'):
                    # é‡è¦æ•°æ®è¡Œï¼Œä½¿ç”¨ç²—ä½“
                    story.append(Paragraph(f"<b>{line}</b>", styles['Normal']))
                elif line.startswith('=') or line.startswith('-'):
                    # åˆ†éš”çº¿ï¼Œè·³è¿‡
                    continue
                else:
                    story.append(Paragraph(line, styles['Normal']))
        
        story.append(Spacer(1, 20))
        
        # æ·»åŠ å›¾è¡¨ä¿¡æ¯
        if chart_files:
            story.append(Paragraph("å¯è§†åŒ–å›¾è¡¨", heading_style))
            for chart_file in chart_files:
                if os.path.exists(chart_file):
                    # æ·»åŠ å›¾è¡¨æ ‡é¢˜
                    chart_name = os.path.basename(chart_file).replace('.png', '').replace('_', ' ').title()
                    story.append(Paragraph(f"<b>{chart_name}</b>", styles['Normal']))
                    
                    # æ·»åŠ å›¾è¡¨å›¾ç‰‡
                    try:
                        img = Image(chart_file, width=6*inch, height=4*inch)
                        story.append(img)
                        story.append(Spacer(1, 12))
                    except Exception as e:
                        story.append(Paragraph(f"å›¾è¡¨åŠ è½½å¤±è´¥: {str(e)}", styles['Normal']))
        
        # æ·»åŠ æ€»ç»“
        story.append(Spacer(1, 20))
        story.append(Paragraph("æ€»ç»“", heading_style))
        summary_text = """
        æœ¬æŠ¥å‘ŠåŸºäºæä¾›çš„æ•°æ®æ–‡ä»¶è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼ŒåŒ…æ‹¬æ•°æ®æ¦‚è§ˆã€è¶‹åŠ¿åˆ†æã€å¼‚å¸¸æ£€æµ‹å’Œå¯è§†åŒ–å±•ç¤ºã€‚
        å¦‚éœ€æ›´è¯¦ç»†çš„åˆ†ææˆ–æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶è”ç³»ã€‚
        """
        story.append(Paragraph(summary_text, styles['Normal']))
        
        # æ„å»ºPDF
        doc.build(story)
        
        logging.info(f"PDFæŠ¥å‘Šå·²ç”Ÿæˆ: {pdf_path}")
        return pdf_path
        
    except Exception as e:
        logging.error(f"ä½¿ç”¨ReportLabç”ŸæˆPDFå¤±è´¥: {str(e)}")
        raise


def _generate_html_report(filename: str, text_analysis: str, chart_files: List[str], report_title: str) -> str:
    """ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Šå†…å®¹"""
    html_content = f"""
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>{report_title}</title>
        <style>
            body {{
                font-family: 'Microsoft YaHei', 'SimHei', Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 20px;
                background-color: #f5f5f5;
            }}
            .container {{
                max-width: 1200px;
                margin: 0 auto;
                background: white;
                padding: 30px;
                border-radius: 10px;
                box-shadow: 0 0 20px rgba(0,0,0,0.1);
            }}
            h1 {{
                color: #2c3e50;
                text-align: center;
                border-bottom: 3px solid #3498db;
                padding-bottom: 10px;
                margin-bottom: 30px;
            }}
            h2 {{
                color: #34495e;
                border-left: 4px solid #3498db;
                padding-left: 15px;
                margin-top: 30px;
            }}
            .file-info {{
                background: #ecf0f1;
                padding: 15px;
                border-radius: 5px;
                margin: 20px 0;
            }}
            .analysis-content {{
                white-space: pre-line;
                background: #f8f9fa;
                padding: 20px;
                border-radius: 5px;
                border-left: 4px solid #28a745;
            }}
            .chart-container {{
                text-align: center;
                margin: 30px 0;
                padding: 20px;
                background: #fff;
                border: 1px solid #ddd;
                border-radius: 5px;
            }}
            .chart-container img {{
                max-width: 100%;
                height: auto;
                border-radius: 5px;
            }}
            .chart-title {{
                font-weight: bold;
                color: #2c3e50;
                margin-bottom: 15px;
                font-size: 16px;
            }}
            .summary {{
                background: #e8f5e8;
                padding: 20px;
                border-radius: 5px;
                border-left: 4px solid #28a745;
                margin-top: 30px;
            }}
            .footer {{
                text-align: center;
                color: #7f8c8d;
                margin-top: 40px;
                padding-top: 20px;
                border-top: 1px solid #ecf0f1;
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>{report_title}</h1>
            
            <div class="file-info">
                <h2>æ•°æ®æ–‡ä»¶ä¿¡æ¯</h2>
                <p><strong>æ–‡ä»¶å:</strong> {os.path.basename(filename)}</p>
                <p><strong>åˆ†ææ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <p><strong>æ•°æ®è¡Œæ•°:</strong> {len(_safe_read_csv(filename))}</p>
            </div>
            
            <h2>åˆ†æç»“æœ</h2>
            <div class="analysis-content">{text_analysis}</div>
            
            <h2>å¯è§†åŒ–å›¾è¡¨</h2>
    """
    
    # æ·»åŠ å›¾è¡¨
    if chart_files:
        for chart_file in chart_files:
            if os.path.exists(chart_file):
                chart_name = os.path.basename(chart_file).replace('.png', '').replace('_', ' ').title()
                html_content += f"""
                <div class="chart-container">
                    <div class="chart-title">{chart_name}</div>
                    <img src="{os.path.basename(chart_file)}" alt="{chart_name}">
                </div>
                """
    else:
        html_content += "<p>æš‚æ— å›¾è¡¨ç”Ÿæˆã€‚</p>"
    
    html_content += """
            <div class="summary">
                <h2>æ€»ç»“</h2>
                <p>æœ¬æŠ¥å‘ŠåŸºäºæä¾›çš„æ•°æ®æ–‡ä»¶è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼ŒåŒ…æ‹¬æ•°æ®æ¦‚è§ˆã€è¶‹åŠ¿åˆ†æã€å¼‚å¸¸æ£€æµ‹å’Œå¯è§†åŒ–å±•ç¤ºã€‚å¦‚éœ€æ›´è¯¦ç»†çš„åˆ†ææˆ–æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶è”ç³»ã€‚</p>
            </div>
            
            <div class="footer">
                <p>æŠ¥å‘Šç”±æ•°æ®åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ</p>
            </div>
        </div>
    </body>
    </html>
    """
    
    return html_content


def _convert_html_to_pdf(html_content: str, output_dir: str, report_title: str) -> str:
    """å°†HTMLå†…å®¹è½¬æ¢ä¸ºPDF"""
    try:
        # åˆ›å»ºHTMLæ–‡ä»¶
        html_filename = f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        html_path = os.path.join(output_dir, html_filename)
        
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        # è½¬æ¢ä¸ºPDF
        pdf_filename = f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        pdf_path = os.path.join(output_dir, pdf_filename)
        
        # ä½¿ç”¨WeasyPrintè½¬æ¢
        HTML(string=html_content).write_pdf(pdf_path)
        
        logging.info(f"HTMLè½¬PDFæŠ¥å‘Šå·²ç”Ÿæˆ: {pdf_path}")
        return pdf_path
        
    except Exception as e:
        logging.error(f"HTMLè½¬PDFå¤±è´¥: {str(e)}")
        raise
